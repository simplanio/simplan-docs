{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Simplan - Simple Execution Planner","text":"<p>Simplan framework offers a way to auther LowCode/NoCode operations in a simple way. It is based on the concept of a plan, which is a set of operations that are executed in a specific order. The operations are defined in a HOCON file and can be executed by the Simplan framework. </p> <p>The framework is built around the concept of operators. An operator is intended to perform an operation which is a unit of work the framework can perform. Operators can be written to abstract complex functionalities within themselves and expose certain configurations to customize their behavior. These operators are grouped to form tasks and it can be configured to run in a certain order to collectively perform the intended outcome. It provides dependency chaining, operatior/task/application level metrics, monitoring, and quality control with circuit breakers before and after performing each operation definition. A rich definition grammar enforces a quick and consistent way to define operations and its dependencies.</p>"},{"location":"#featuresbenefits","title":"Features/Benefits","text":"<ul> <li>Config Driven (Low/No code)</li> <li>Configurable and Pluggable Operators</li> <li>Maintains and abstracts application initialization execution context.<ul> <li>Eg: In the case of Spark, SparkSession and SparkContext and in case of flink, StreamExecutionEnvironment and StreamTableEnvironment etc</li> </ul> </li> <li>Connect to any JDBC sources and execute like Redshift, Athena, Presto, etc</li> <li>Built-In Quality control with circuit breakers pre/post each operator execution.</li> <li>Lineage, Observability, and Metrics tracking are built into the framework</li> <li>Metrics are emitted as a specific category within log4j which can be handled by any appender and published to an aggregation tool the user chooses like Elasticsearch/Opensearch/Splunk.</li> <li>Maintains XComs which maintains operator response states, and provides ability to cross communicate between operations.</li> <li> <p>System-level features can be added as additive functions (extend Support trait)</p> <ul> <li>Eg: IDPS Support can be added by just mentioning it as follows. <pre><code>val context = new ConsoleAppContext(config) with IDPSSupport\n</code></pre></li> </ul> </li> <li> <p>A lot of utils for Config parsing, Managing Execution context, Json Mapping, Exception handling, Execution tracking, metrics publishing, AWS-specific services like STS and S3, abstractions to handle Local/Hadoop/S3 file operations, to name a few.</p> </li> </ul>"},{"location":"#simplan-for-data-process-authoring","title":"Simplan for Data Process Authoring","text":"<p>Simplan Spark and Simplan Flink are 2 different implementations of Simplan framework for data processing built on top of Apache Spark and Apache Flink respectively. These implementations are built to support the same set of features and functionalities on top of correspnding execution environments. Simplan's common configuration structure in thtended to allow easy migration of plans between different execution environments.</p>"},{"location":"#simplan-for-orchestration","title":"Simplan for Orchestration","text":"<p>Superglue uses Simplan framework for orchestration all its jobs. SG-Client is a simplan based appliation which is used to orchestrate all the jobs in Superglue.</p>"},{"location":"#presentations","title":"Presentations","text":""},{"location":"#simplan-dataai-summit","title":"Simplan @ DataAI Summit","text":""},{"location":"#communitysupport","title":"Community/Support","text":"<ul> <li>Join Simplan Slack Channel #simplan-community</li> </ul>"},{"location":"ContributionGuide/","title":"ContributionGuide","text":""},{"location":"ContributionGuide/#contribution-guide","title":"Contribution Guide","text":"<p>This guide is intended to help you get started contributing to the project. It is not intended to be a comprehensive guide to contributing to the project, but rather a guide to getting started. If you have any questions, please feel free to reach out to #simplan-community.</p>"},{"location":"ContributionGuide/#code-of-conduct","title":"Code of Conduct","text":"<p>Projects are \"living\" and constantly evolving. Contributions in the form of issues and pull requests are welcomed and encouraged. When you contribute, you explicitly say you are part of the community and abide by its Code of Conduct.</p> <p>At Intuit, we foster a kind, respectful, inclusive, harassment-free cooperative community. Our community works to:</p> <ul> <li>Be kind and respectful;</li> <li>Act as a global community;</li> <li>Conduct ourselves professionally.</li> <li>As members of this community, we will not tolerate behaviors including, but not limited to:<ul> <li>Violent threats or language;</li> <li>Discriminatory or derogatory jokes or language;</li> <li>Public or private harassment of any kind;</li> <li>Other conduct considered inappropriate in a professional setting.</li> </ul> </li> </ul>"},{"location":"ContributionGuide/#how-to-contribute","title":"How to Contribute","text":"<ul> <li>Clone the inner source repository to your local machine. Framework | Spark</li> <li>Create a branch for your changes.</li> <li>Make your changes, test it and push your changes to the branch in inner source repository.</li> <li>Create a pull request to the develop branch of the inner source repository.</li> <li>Once your pull request is approved by Simplan Maintainers, it will be merged into the develop branch of the inner source repository.</li> <li>The develop branch of the inner source repository will be synced with the develop branch of the main repository.</li> <li>Your changes will be available in the main repository and will be available in the next release.</li> </ul>"},{"location":"ContributionGuide/#reporting-bugs","title":"Reporting Bugs","text":"<p>If you find a bug, please report it by opening a new issue. Please include as much information as possible, including the version of the project you are using, and execution context where the application is running. If you are able to reproduce the bug, please include a test case or an executable test case.</p>"},{"location":"framework/","title":"Simplan Framework","text":"<p>Simplan Framework is the underlying framework which he</p>"},{"location":"framework/concepts/Operators/","title":"Operators","text":""},{"location":"implementations/flink/","title":"Simplan for Apache Flink","text":"<p>For generating customer value from data, Data workers need to process large volumes of batch and streaming data. Separate codebase are maintained for Batch and Streaming modes which leads to siloed implementations for common data processing patterns. This leads to duplicate efforts from implementation to maintenance, hampering productivity.</p> <p>Users will be able to provide business logic as operators in a config file and the framework will take care of the rest. The framework will take care of the execution of these operators and provide the results to the user. The framework will also provide the lineage of the data and the metrics of the execution.</p> <p>Simplan Spark is an implementation of Simplan framework which adds SparkApplicationContext to the framework. Refer Simplan Framework documentation to learn more.</p>"},{"location":"implementations/flink/#tech-stack","title":"Tech Stack","text":""},{"location":"implementations/flink/#features","title":"Features","text":"<ul> <li>Config Driven (Low/No code)</li> <li>Pluggable/Reusable operators for common processing tasks</li> <li>Batch and Streaming workloads</li> <li>External Integrations : Redshift, Athena, Kafka etc</li> <li>Built-In Quality control with circuit breakers.</li> <li>Lineage, Observability, and Metrics tracking.</li> <li>Integration for Intuit services like IDPS, Config Services, etc</li> <li>Improves developer productivity by 10-100 times</li> <li>Improves code quality, maintainability and reduces duplication</li> </ul>"},{"location":"implementations/flink/#presentations","title":"Presentations","text":""},{"location":"implementations/flink/#simplan-spark-dataai-summit","title":"Simplan Spark @ DataAI Summit","text":""},{"location":"implementations/flink/#simplan-community","title":"Simplan Community","text":"<ul> <li>Join Simplan Slack Channel #simplan-community</li> <li>Ask questions on StackOverflow using tag <code>simplan-spark</code></li> </ul>"},{"location":"implementations/flink/#other-simplan-implementations","title":"Other Simplan Implementations","text":"<ul> <li>Simplan Spark</li> <li>Simplan Presto</li> </ul>"},{"location":"implementations/flink/ContributionGuide/","title":"Contribution Guide","text":"<p>Simplan Spark follows the same contribution process as the rest of the Simplan projects.</p> <p>Please refer to the Contribution Guide for more details.</p> <p>Simplan Spark Inner source repo : https://github.intuit.com/Simplan-InnerSource/simplan-spark</p>"},{"location":"implementations/flink/DeploymentOptions/","title":"Deployment Options","text":""},{"location":"implementations/flink/DeploymentOptions/#simplan-launcher","title":"Simplan Launcher","text":"<p>Simplan launcher is a ready to go Simplan deployment which can accept authoring logic (as config files) as command line arguments and execute it without any need for build or deployment.</p> <p>Maintained by: Simplan Team  Pros: No Build/Deployment process required.  Cons: Supports only built in Operations. </p>"},{"location":"implementations/flink/DeploymentOptions/#simplan-launcher-custom-operators","title":"Simplan Launcher + Custom Operators","text":"<p>Works similar to the option above(Simplan Launcher), custom operators need to be built as an independent jar and provided in classpath using <code>--jars</code> during spark-submit.</p> <p>Maintained by: Launcher (Simplan Team) and Custom Operators (customer team)  Pros: Custom operators can be used.  Cons: CI/CD of custom operator needs to be maintained by the customer.</p>"},{"location":"implementations/flink/DeploymentOptions/#custom-launcher","title":"Custom Launcher","text":"<p>Users will be able to write a custom launcher which can be used to execute Simplan jobs. This will be a standalone jar which can be executed using spark-submit. This jar will be responsible for building the Simplan job and submitting it to spark.</p> <p>The following maven dependency needs to be added to the custom launcher project.</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;com.simplan&lt;/groupId&gt;\n&lt;artifactId&gt;simplan-core&lt;/artifactId&gt;\n&lt;version&gt;0.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Maintained by: Customer Team  Pros: Custom operators can be used.  Cons: CI/CD of custom launcher needs to be maintained by the customer.</p>"},{"location":"implementations/flink/GettingStarted/","title":"Getting Started","text":"<p>Simplan Flink is available as a paved path solution integrated with Stream Processing Platform. </p> <p>To get started, please follow the instructions below.</p>"},{"location":"implementations/flink/GettingStarted/#create-a-new-processor","title":"Create a new Processor","text":"<p>Go to Dev Portal and create a new Data Processor. </p> <p>In the second screen of the wizard, select</p> <ul> <li>Processor Type as <code>Stream Processor</code></li> <li>Runtime Framework as <code>Flink</code></li> <li>Source Language as <code>Simplan - Low code authoring</code></li> </ul> <p></p>"},{"location":"implementations/flink/operators/BuildYourOperator/","title":"Build Your Operator","text":""},{"location":"implementations/flink/operators/BuildYourOperator/#overview","title":"Overview","text":"<p>Simplan framework is built around the concept of operators. An operator is intended to perform an operation which is a unit of work the framework can perform. Learn more about operators in the Operators section.</p> <p>In this section, we will learn how to build an operator for SparkAppContext. We will build an operator that will execute a Spark SQL statement on a DataFrame. It is a wrapper around the Spark SQL API. Any Spark SQL statement can be executed using this transformation. Any existing table in Hive metastore can be queried using this transformation. The transformation can also be used to create a new table in Hive metastore.</p> <p>If you just like to see the code, you can find it here.</p> <p>More examples of operators can be found here.</p>"},{"location":"implementations/flink/operators/BuildYourOperator/#defining-a-configuration-class","title":"Defining a configuration class","text":"<p>The config class is used to define the configuration parameters for the operator. A case class can be defined in a specific structure to emulate the configuration parameters. </p> <p><pre><code>case class WhereConditionOperatorConfig(source: String, condition: String)\n</code></pre> | Parameter | Description | Required | Default | |-----------|-------------|----------|---------| | source | Source on which the filteration is to be performed | Yes | NA | | condition | Condition to Filter | Yes | NA |</p>"},{"location":"implementations/flink/operators/BuildYourOperator/#defining-the-operator","title":"Defining the operator","text":"<p>To create an operator targetted for SparkAppContext in Simplan you need to extend SparkOperator class provided by Simplan framework and implements process(SparkOperatorRequest) method. The process method takes a <code>SparkOperatorRequest</code> as input and returns a <code>SparkOperatorResponse</code>. The Operator accepts a <code>SparkAppContext</code> as a constructor arguement which contains all the context information required to execute the operator.</p> <p><pre><code>package com.intuit.data.simplan.spark.core.operators.transformations\nimports ...\nclass FilteringOperator(appContext: SparkAppContext) extends SparkOperator(appContext) {\noverride def process(request: SparkOperatorRequest): SparkOperatorResponse = {\nval config: WhereConditionOperatorConfig = request.parseConfigAs[WhereConditionOperatorConfig]\nval sourceDataframe: DataFrame = request.dataframes(config.source)\nval filteredDataframe = sourceDataframe.where(config.condition)\nSparkOperatorResponse(request.taskName, filteredDataframe)\n}\n}\n</code></pre> <code>SparkOperatorRequest</code> contains information of all the dataframes that are available in the context. The operator can use the dataframes to perform the operation. The operator can also use the config to perform the operation. The operator can return a SparkOperatorResponse which contains the name of the task and the dataframe that is generated as a result of the operation.</p>"},{"location":"implementations/flink/operators/BuildYourOperator/#operator-configuration","title":"Operator Configuration","text":"<p>The above operator can be configured in the task as shown below:</p> <pre><code>TaskName {\n  action {\n    operator = com.intuit.data.simplan.spark.core.operators.transformations.FilteringOperator\n    config = {\n        source = source_table\n        condition = condition\n    }\n  }\n}\n</code></pre>"},{"location":"implementations/flink/operators/flink/sinks/stream/KafkaStreamSink/","title":"Kafka Stream Sink","text":"<p>The Kafka Stream Sink operator writes records to a Kafka Stream. </p>"},{"location":"implementations/flink/operators/flink/sinks/stream/KafkaStreamSink/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name KafkaStreamingSource com.intuit.data.simplan.spark.core.operators.sources.stream.KafkaStreamingSource"},{"location":"implementations/flink/operators/flink/sinks/stream/KafkaStreamSink/#configuration","title":"Configuration","text":"<pre><code>    ProduceToKafka {\naction {\noperator = KafkaStreamingSink\nconfig {\nsource = SourceDataFrame\noutputMode = COMPLETE | UPDATE | APPEND\noptions {\n\"kafka.bootstrap.servers\" = \"localhost:9092\"\ntopic = topicName\ncheckpointLocation: /path/to/checkpoint\n}\n}\n}\n}\n</code></pre>"},{"location":"implementations/flink/operators/flink/sinks/stream/KafkaStreamSink/#parameters","title":"Parameters","text":"Parameter Description Required Default source Name of the source DataFrame. Yes NA outputMode Output mode of the stream. Yes NA options Options for the Kafka Stream. Yes NA"},{"location":"implementations/flink/operators/flink/sources/stream/KafkaStreamingSource/","title":"Kafka Stream Reader","text":"<p>Reads records from a Kafka Stream and converts each record into a Structured Record. The schema of the Structured Record can be provided using the schema property. Schema is provided as Schema Qualifed Param. </p>"},{"location":"implementations/flink/operators/flink/sources/stream/KafkaStreamingSource/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name KafkaStreamingSource com.intuit.data.simplan.spark.core.operators.sources.stream.KafkaStreamingSource"},{"location":"implementations/flink/operators/flink/sources/stream/KafkaStreamingSource/#configuration","title":"Configuration","text":"<pre><code>    ReadVendorStream {\naction {\noperator = KafkaStreamingSource\nconfig = {\nformat = JSON\npayloadSchema = schemaJson(\"path/to/payloadSchema.schema.json\")\nheaderSchema = schemaJson(\"path/to/headerSchema.schema.json\")\nheaderField = \"key\"\ntableType = &lt;TEMP | NONE&gt;\nwatermark ={\neventTime = fieldName\ndelayThreshold = \"1 minute\"\n}\nparseMode = &lt;ALL_PARSED | PAYLOAD_ONLY | HEADER_ONLY | ALL&gt;\noptions = {\n\"kafka.bootstrap.servers\" = \"server1:port, server2:port\"\nincludeHeaders = true\nsubscribe = topic-name\n...\n}\n}\n}\n}\n</code></pre>"},{"location":"implementations/flink/operators/flink/sources/stream/KafkaStreamingSource/#parameters","title":"Parameters","text":"Parameter Description Required Default format Format of the data in the stream. Yes NA payloadSchema Schema of the payload in the stream. Yes NA headerSchema Schema of the header in the stream. No NA headerField Name of the field in the header schema that contains the header. No key tableType Type of table that gets created.  NONE - Maintained in memory as dataframe(Not usable in SQL) TEMP - Temperory table in Memory(Usable in SQL) No NONE watermark Watermark configuration for the stream. No NA parseMode Mode of parsing the stream.  ALL_PARSED - Both header and payload are parsed and available in the dataframe.  PAYLOAD_ONLY - Only payload is parsed and available in the dataframe.  HEADER_ONLY - Only header is parsed and available in the dataframe.  ALL - Both header and payload are parsed and available in the dataframe. No PAYLOAD_ONLY options Options to be passed to the Kafka reader.  Any Kafka property can be passed prefixed with kafka.* Yes NA"},{"location":"implementations/flink/operators/flink/sources/stream/KafkaStreamingSource/#sample-configuration","title":"Sample Configuration","text":"<pre><code>    bands {\n  action {\n    operator = KafkaStreamReader\n    config = {\n      format = JSON\n      payloadSchema = schemaJson(\"schema.json\")\n      headerSchema = schemaJson(\"header.schema.json\")\n      parseMode = ALL_PARSED\n      options = {\n        \"kafka.bootstrap.servers\" = \"b-3.eventbusmsk-sbseg-e2e.hz7ee1.c3.kafka.us-west-2.amazonaws.com:9094, b-4.eventbusmsk-sbseg-e2e.hz7ee1.c3.kafka.us-west-2.amazonaws.com:9094, b-2.eventbusmsk-sbseg-e2e.hz7ee1.c3.kafka.us-west-2.amazonaws.com:9094\"\n        \"kafka.cluster.env\" = \"e2e\"\n        \"kafka.cluster.name\" = \"ebus_sbseg_sl\"\n        \"kafka.cluster.region\" = \"uw2\"\n        \"kafka.group.id\" = \"test111\"\n        \"kafka.auto.commit.interval.ms\" = \"1000\"\n        \"kafka.ssl.keystore.location\" = \"/Users/tabraham1/Intuit/Development/certifates/local-certificate.jks\"\n        \"kafka.ssl.keystore.password\" = \"&lt;password&gt;\"\n        \"kafka.security.protocol\" = \"ssl\"\n        \"includeHeaders\" = \"true\"\n        subscribe = \"e2e.qbo.commerce.vendormanagement.vendor.v1\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"implementations/flink/operators/flink/transformations/DropDuplicates/","title":"Drop duplicates Transformation","text":"<p>Drop Duplicates is a transformation that allows you to remove duplicate rows from a a set of rows.</p>"},{"location":"implementations/flink/operators/flink/transformations/DropDuplicates/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name DropDuplicatesOperator com.intuit.data.simplan.spark.core.operators.transformations.DropDuplicatesOperator"},{"location":"implementations/flink/operators/flink/transformations/DropDuplicates/#compatability","title":"Compatability","text":"Batch Streaming Yes Yes"},{"location":"implementations/flink/operators/flink/transformations/DropDuplicates/#configuration","title":"Configuration","text":"<pre><code>    DropDuplicateVendorEvents {\n      action {\n        operator = DropDuplicatesOperator\n        config = {\n          source = ProjectRequiredFields\n          primaryKeyColumns = [fieldNamew]\n          dropWindowDuration = \"10 minutes\"\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/flink/operators/flink/transformations/DropDuplicates/#parameters","title":"Parameters","text":"Parameter Description Required Default source Name of the source DataFrame. Yes NA projections List of columns to be selected. Yes NA"},{"location":"implementations/flink/operators/flink/transformations/Filtering/","title":"Filtering Transformation","text":"<p>Filtering is a transformation that allows you to filter out rows from a DataFrame based on a condition. The condition can be a SQL expression which leads to a boolean outcome.</p>"},{"location":"implementations/flink/operators/flink/transformations/Filtering/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name FilteringOperator com.intuit.data.simplan.spark.core.operators.transformations.FilteringOperator"},{"location":"implementations/flink/operators/flink/transformations/Filtering/#compatability","title":"Compatability","text":"Batch Streaming Yes Yes"},{"location":"implementations/flink/operators/flink/transformations/Filtering/#configuration","title":"Configuration","text":"<pre><code> Filtering {\n      action {\n        operator = FilteringOperator\n        config = {\n          source = SourceDataFrame\n          condition = \"event_type = 'EventToBeFiltered'\"\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/flink/operators/flink/transformations/Filtering/#parameters","title":"Parameters","text":"Parameter Description Required Default source Name of the source DataFrame. Yes NA condition SQL expression that leads to a boolean outcome. Yes NA"},{"location":"implementations/flink/operators/flink/transformations/GroupedAggregationOperator/","title":"Grouped Aggregation Transformation","text":"<p>Grouped aggregation is a transformation that allows you to aggregate rows from a DataFrame based on a group by condition. Expressions can be used to aggregate the rows. Any expressions supported by Spark SQL can be used.</p>"},{"location":"implementations/flink/operators/flink/transformations/GroupedAggregationOperator/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name GroupedAggregationOperator com.intuit.data.simplan.spark.core.operators.transformations.GroupedAggregationOperator"},{"location":"implementations/flink/operators/flink/transformations/GroupedAggregationOperator/#compatability","title":"Compatability","text":"Batch Streaming Yes Yes"},{"location":"implementations/flink/operators/flink/transformations/GroupedAggregationOperator/#configuration","title":"Configuration","text":"<pre><code>    GroupedAggregation {\n      action {\n        operator = GroupedAggregationOperator\n        config = {\n          source = sourceDataFrame\n          grouping = [groupField]\n          aggs {\n            aggField1 = aggregationExpression1\n            aggField2 = aggregationExpression2\n          }\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/flink/operators/flink/transformations/GroupedAggregationOperator/#parameters","title":"Parameters","text":"Parameter Description Required Default source Name of the source DataFrame. Yes NA grouping List of fields to group by. Yes NA aggs Map of fields to aggregate and the aggregation expression. Yes NA"},{"location":"implementations/flink/operators/flink/transformations/GroupedAggregationOperator/#example","title":"Example","text":"<p>Calculating 2 aggregations to find <code>numberjobs</code> and <code>totalNumberOfStatements</code> grouped by a field called <code>asset</code>.</p> <pre><code>    NumberOfVendorsLifetime {\n      action {\n        operator = GroupedAggregationOperator\n        config = {\n          source = ProjectRequiredFields\n          grouping = [asset]\n          aggs {\n            numberOfJobs = count(appName)\n            totalNumberOfStatemts = sum(numOfStatements)\n          }\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/flink/operators/flink/transformations/Projections/","title":"Projections Transformation","text":"<p>Projections is a transformation that allows you to select a subset of columns from a DataFrame. The columns can be selected by name or by using a SQL expression.</p>"},{"location":"implementations/flink/operators/flink/transformations/Projections/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name ProjectionOperator com.intuit.data.simplan.spark.core.operators.transformations.ProjectionOperator"},{"location":"implementations/flink/operators/flink/transformations/Projections/#compatability","title":"Compatability","text":"Batch Streaming Yes Yes"},{"location":"implementations/flink/operators/flink/transformations/Projections/#configuration","title":"Configuration","text":"<pre><code>    ProjectRequiredFields {\n      action {\n        operator = ProjectionOperator\n        config = {\n          source = SourceDataFrame\n          projections = [\n            \"value.id.accountId as accountId\",\n            \"value.id.entityId as entityId\",\n            \"value.active as active\",\n            \"headers.entityChangeAction as entityChangeAction\",\n            \"headers.idempotenceKey as idempotenceKey\",\n            \"current_timestamp() as timestamp\"\n          ]\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/flink/operators/flink/transformations/Projections/#parameters","title":"Parameters","text":"Parameter Description Required Default source Name of the source DataFrame. Yes NA projections List of columns to be selected. Yes NA"},{"location":"implementations/flink/operators/flink/transformations/SparkSql/","title":"SQL Transformations","text":"<p>SqlStatementExecutor is a Spark SQL transformation that executes a Spark SQL statement on a DataFrame. It is a wrapper around the Spark SQL API. Any Spark SQL statement can be executed using this transformation. Any existing table in Hive metastore can be queried using this transformation. The transformation can also be used to create a new table in Hive metastore.</p>"},{"location":"implementations/flink/operators/flink/transformations/SparkSql/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name SqlStatementExecutor com.intuit.data.simplan.spark.core.operators.transformations.SqlStatementOperator"},{"location":"implementations/flink/operators/flink/transformations/SparkSql/#compatability","title":"Compatability","text":"Batch Streaming Yes Yes"},{"location":"implementations/flink/operators/flink/transformations/SparkSql/#configuration","title":"Configuration","text":"<pre><code>    TaskName {\n      action {\n        operator = SqlStatementExecutor\n        config = {\n            table = \"table_name\"\n            tableType = &lt;TEMP | MANAGED | NONE&gt;\n            sql = \"SELECT field FROM schema.table_name\"\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/flink/operators/flink/transformations/SparkSql/#parameters","title":"Parameters","text":"Parameter Description Required Default table Name of the table to be created. No NA tableType Type of table that gets created.  NONE - Maintained in memory as dataframe(Not usable in SQL) MANAGED - As managed table in Hive  TEMP - Temperory table in Memory(Usable in SQL) No NONE sql Spark SQL statement to be executed. Yes NA"},{"location":"implementations/spark/","title":"SimPlan for Apache Spark","text":"<p>For generating customer value from data, Data workers need to process large volumes of batch and streaming data. Separate codebase are maintained for Batch and Streaming modes which leads to siloed implementations for common data processing patterns. This leads to duplicate efforts from implementation to maintenance, hampering productivity.</p> <p>Users will be able to provide business logic as operators in a config file and the framework will take care of the rest. The framework will take care of the execution of these operators and provide the results to the user. The framework will also provide the lineage of the data and the metrics of the execution.</p> <p>Simplan Spark is an implementation of Simplan framework which adds SparkApplicationContext to the framework. Refer Simplan Framework documentation to learn more.</p>"},{"location":"implementations/spark/#tech-stack","title":"Tech Stack","text":""},{"location":"implementations/spark/#features","title":"Features","text":"<ul> <li>Config Driven (Low/No code)</li> <li>Pluggable/Reusable operators for common processing tasks</li> <li>Batch and Streaming workloads</li> <li>External Integrations : Redshift, Athena, Kafka etc</li> <li>Built-In Quality control with circuit breakers.</li> <li>Lineage, Observability, and Metrics tracking.</li> <li>Integration for Intuit services like IDPS, Config Services, etc</li> <li>Improves developer productivity by 10-100 times</li> <li>Improves code quality, maintainability and reduces duplication</li> </ul>"},{"location":"implementations/spark/#presentations","title":"Presentations","text":""},{"location":"implementations/spark/#simplan-dataai-summit","title":"Simplan @ DataAI Summit","text":""},{"location":"implementations/spark/#simplan-community","title":"Simplan Community","text":"<ul> <li>Join Simplan Slack Channel #simplan-community</li> <li>Ask questions on StackOverflow using tag <code>simplan-spark</code></li> </ul>"},{"location":"implementations/spark/#other-simplan-implementations","title":"Other Simplan Implementations","text":"<ul> <li>Simplan Flink</li> <li>Simplan Presto</li> </ul>"},{"location":"implementations/spark/ContributionGuide/","title":"Contribution Guide","text":"<p>Simplan Spark follows the same contribution process as the rest of the Simplan projects.</p> <p>Please refer to the Contribution Guide for more details.</p> <p>Simplan Spark Inner source repo : https://github.intuit.com/Simplan-InnerSource/simplan-spark</p>"},{"location":"implementations/spark/GettingStarted/","title":"Getting Started","text":"<p>Lets go through the steps to get started with the Simplan Spark. This document provides instructions to setup and run the Simplan Spark Reference Implementation (RI) project. The project provides a sample implementation of Simplan Spark. The project can be used as a reference to understand how to use Simplan Spark framework. We will setup Simplan spark on a local machine and then run a simple example for both batch and streaming mode.</p>"},{"location":"implementations/spark/GettingStarted/#setup-instructions","title":"Setup Instructions","text":""},{"location":"implementations/spark/GettingStarted/#step-1-clone-simplan-spark-ri","title":"Step 1: Clone Simplan Spark RI","text":"<p>Clone the Simplan Spark Refererence implementation repository to a direcrory of your choice https://github.intuit.com/Simplan/simplan-spark-ri/</p>"},{"location":"implementations/spark/GettingStarted/#step-2-update-base-path","title":"Step 2: Update base path","text":"<p>Update the path of the workspace path in the file simplan-spark-ri/src/main/resources/conf/busines-logic.simplan.conf to the path of the directory where you cloned the repository.</p>"},{"location":"implementations/spark/GettingStarted/#step-3-open-the-project-in-intellij","title":"Step 3: Open the project in IntelliJ","text":"<p>Open the project in IntelliJ. The project should be opened as a Maven project. If not, select the pom.xml file and select \"Open as Maven project\".</p>"},{"location":"implementations/spark/GettingStarted/#step-4-run-the-project","title":"Step 4: Run the project","text":"<p>Run the project by selecting the class com.intuit.data.simplan.ri.spark.SimPlanSparkLauncherLocal and selecting <code>Run SimPlanSparkLauncherLocal</code> using the play button in Intellij.</p> <p>Sample can be run in batch and streaming modes. You can choose the mode by passing the appropriate batch-io.simplan.conf or streaming-io.simplan.conf file as the argument to the SimPlanSparkLauncherLocal class.</p>"},{"location":"implementations/spark/GettingStarted/#step-5-verifying-the-output","title":"Step 5: Verifying the output","text":"<ul> <li>For batch mode : The output will be present in <code>&lt;projectbase&gt;/src/main/resources/output/SimplanRI</code></li> <li>For Streaming mode : The data has to be produced to simplan-ri-source topic and output will be produced to simplan-ri-sink topic.</li> </ul>"},{"location":"implementations/spark/integrations/bpp/BPPSharedScriptProcessor/","title":"SQL Script in BPP","text":"<p>Simplan can be used to execute an SQL Script on BPP.</p>"},{"location":"implementations/spark/integrations/bpp/BppCustomProcessor/","title":"Simplan Spark on BPP","text":"<p>Create a custom BPP Processor</p>"},{"location":"implementations/spark/integrations/bpp/BppSharedProcessor/","title":"Simplan Shared Processor in BPP","text":"<p>Any Simplan config can be run using this processor.</p>"},{"location":"implementations/spark/integrations/bpp/GettingStarted/","title":"Simplan Spark on BPP","text":"<p>Documenation for Simplan Spark on BPP</p>"},{"location":"implementations/spark/integrations/superglue/SuperglueLauncher/","title":"Simplan Launcher for Superglue","text":""},{"location":"implementations/spark/integrations/superglue/SuperglueLauncher/#introduction","title":"Introduction","text":"<p>Superglue is a tool for analysts to create and maintain pipelines by just writing Spark or Presto SQLs. It is powered underneeth by Simplan Framework. Users provides a SQL Script via Superglue UI and Simplan Launcher executes the script and performs the required housekeeping tasks.</p>"},{"location":"implementations/spark/operators/BuildYourOperator/","title":"Build Your Operator","text":""},{"location":"implementations/spark/operators/BuildYourOperator/#overview","title":"Overview","text":"<p>Simplan framework is built around the concept of operators. An operator is intended to perform an operation which is a unit of work the framework can perform. Learn more about operators in the Operators section.</p> <p>In this section, we will learn how to build an operator for SparkAppContext. We will build an operator that will execute a Spark SQL statement on a DataFrame. It is a wrapper around the Spark SQL API. Any Spark SQL statement can be executed using this transformation. Any existing table in Hive metastore can be queried using this transformation. The transformation can also be used to create a new table in Hive metastore.</p> <p>If you just like to see the code, you can find it here.</p> <p>More examples of operators can be found here.</p>"},{"location":"implementations/spark/operators/BuildYourOperator/#defining-a-configuration-class","title":"Defining a configuration class","text":"<p>The config class is used to define the configuration parameters for the operator. A case class can be defined in a specific structure to emulate the configuration parameters. </p> <p><pre><code>case class WhereConditionOperatorConfig(source: String, condition: String)\n</code></pre> | Parameter | Description | Required | Default | |-----------|-------------|----------|---------| | source | Source on which the filteration is to be performed | Yes | NA | | condition | Condition to Filter | Yes | NA |</p>"},{"location":"implementations/spark/operators/BuildYourOperator/#defining-the-operator","title":"Defining the operator","text":"<p>To create an operator targetted for SparkAppContext in Simplan you need to extend SparkOperator class provided by Simplan framework and implements process(SparkOperatorRequest) method. The process method takes a <code>SparkOperatorRequest</code> as input and returns a <code>SparkOperatorResponse</code>. The Operator accepts a <code>SparkAppContext</code> as a constructor arguement which contains all the context information required to execute the operator.</p> <p><pre><code>package com.intuit.data.simplan.spark.core.operators.transformations\nimports ...\nclass FilteringOperator(appContext: SparkAppContext) extends SparkOperator(appContext) {\noverride def process(request: SparkOperatorRequest): SparkOperatorResponse = {\nval config: WhereConditionOperatorConfig = request.parseConfigAs[WhereConditionOperatorConfig]\nval sourceDataframe: DataFrame = request.dataframes(config.source)\nval filteredDataframe = sourceDataframe.where(config.condition)\nSparkOperatorResponse(request.taskName, filteredDataframe)\n}\n}\n</code></pre> <code>SparkOperatorRequest</code> contains information of all the dataframes that are available in the context. The operator can use the dataframes to perform the operation. The operator can also use the config to perform the operation. The operator can return a SparkOperatorResponse which contains the name of the task and the dataframe that is generated as a result of the operation.</p>"},{"location":"implementations/spark/operators/BuildYourOperator/#operator-configuration","title":"Operator Configuration","text":"<p>The above operator can be configured in the task as shown below:</p> <pre><code>TaskName {\n  action {\n    operator = com.intuit.data.simplan.spark.core.operators.transformations.FilteringOperator\n    config = {\n        source = source_table\n        condition = condition\n    }\n  }\n}\n</code></pre>"},{"location":"implementations/spark/operators/spark/sinks/batch/AvroBatchSink/","title":"Avro Batch Sink","text":"<p>For Writing to Batch Sink</p> <pre><code>WriteData {\n  action {\n    operator = com.intuit.data.simplan.spark.core.operators.sources.batch.AvroBatchSink\n    config = {\n      source = &lt;file path or previoue operator&gt;\n      location = /output/path\n      options ={\n        &lt;option1&gt; = &lt;value1&gt;\n      }\n    }\n  }\n}\n</code></pre> <p>All options supported by spark can be used.  </p> <p>Example for writing to Avro Batch Sink</p> <pre><code>    finalOutput {\n      action {\n        operator = AvroBatchSink\n        config = {\n          source = SqlOperation\n          location = ${simplan.variables.configBasePath}/output/consolidated/\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/spark/operators/spark/sinks/batch/CSVBatchSink/","title":"CSV Batch Sink","text":"<p>For Writing to Batch Sink</p> <pre><code>ReadData {\n  action {\n    operator = com.intuit.data.simplan.spark.core.operators.sources.batch.CSVBatchSink\n    config = {\n      source = &lt;file path or previoue operator&gt;\n      location = /output/path\n      options ={\n        &lt;option1&gt; = &lt;value1&gt;\n      }\n    }\n  }\n}\n</code></pre> <p>All options supported by spark can be used</p> <p>Example for writing to CSV Batch Sink</p> <pre><code>    finalOutput {\n      action {\n        operator = CSVBatchSink\n        config = {\n          source = SqlOperation\n          location = ${simplan.variables.configBasePath}/output/consolidated/\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/spark/operators/spark/sinks/batch/JsonBatchSink/","title":"Json Batch Sink","text":"<p>For Reading Json Sink</p> <pre><code>ReadData {\n  action {\n    operator = com.intuit.data.simplan.spark.core.operators.sources.batch.JsonBatchSink\n    config = {\n      source = &lt;file path or previoue operator&gt;\n      location = /output/path\n      options ={\n        &lt;option1&gt; = &lt;value1&gt;\n      }\n    }\n  }\n}\n</code></pre> <p>All options supported by spark can be used  </p> <p>Example for writing to JsonBatch Sink</p> <pre><code>    finalOutput {\n      action {\n        operator = JsonBatchSink\n        config = {\n          source = projectionOperation\n          location = ${simplan.variables.configBasePath}/output/consolidated\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/spark/operators/spark/sinks/batch/KafkaBatchSink/","title":"Kafka Batch Sink","text":"<p>For writing to  Kafka  as a batch</p> <p><pre><code>WriteData {\n  action {\n    operator = com.intuit.data.simplan.spark.core.operators.sources.batch.KafkaBatchSink\n    config = {\n      source = &lt;file or previous operator&gt;\n      schema = /path/to/schema.json\n      options ={\n        subscribe = &lt;topic name&gt;\n        &lt;option2&gt; = &lt;value2&gt;\n      }\n    }\n  }\n}\n</code></pre> All optional values supported by spark can be added. Example for Kafka Batch Sink</p> <pre><code> finalOutput {\n      action {\n        operator = KafkaStreamingSink\n        config {\n          source = filteringOperation\n          options {\n            \"kafka.bootstrap.servers\" = \"localhost:9092\"\n            topic = finalOutput\n            checkpointLocation: /Users/tabraham1/Intuit/Development/GitHub/Enterprise/tabraham1/simplan-spark/spark-launcher/src/main/resources/test/kafka_checkpoint\n          }\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/spark/operators/spark/sinks/batch/ParquetBatchSink/","title":"Parquet Batch Sink","text":"<p>For Writing to Batch Sink</p> <pre><code>ReadData {\n  action {\n    operator = com.intuit.data.simplan.spark.core.operators.sources.batch.ParquetBatchSink\n    config = {\n      source = &lt;file path or previoue operator&gt;\n      location = /output/path\n      options ={\n        &lt;option1&gt; = &lt;value1&gt;\n      }\n    }\n  }\n}\n</code></pre> <p>All options supported by spark can be used  </p> <p>Example for writing to Parquet Batch Sink</p> <pre><code>    finalOutput {\n      action {\n        operator = ParquetBatchSink\n        config = {\n          source = SqlOperation\n          location = ${simplan.variables.configBasePath}/output/consolidated/\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/spark/operators/spark/sinks/stream/KafkaStreamSink/","title":"Kafka Stream Sink","text":"<p>The Kafka Stream Sink operator writes records to a Kafka Stream. </p>"},{"location":"implementations/spark/operators/spark/sinks/stream/KafkaStreamSink/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name KafkaStreamingSource com.intuit.data.simplan.spark.core.operators.sources.stream.KafkaStreamingSource"},{"location":"implementations/spark/operators/spark/sinks/stream/KafkaStreamSink/#configuration","title":"Configuration","text":"<pre><code>    ProduceToKafka {\naction {\noperator = KafkaStreamingSink\nconfig {\nsource = SourceDataFrame\noutputMode = COMPLETE | UPDATE | APPEND\noptions {\n\"kafka.bootstrap.servers\" = \"localhost:9092\"\ntopic = topicName\ncheckpointLocation: /path/to/checkpoint\n}\n}\n}\n}\n</code></pre>"},{"location":"implementations/spark/operators/spark/sinks/stream/KafkaStreamSink/#parameters","title":"Parameters","text":"Parameter Description Required Default source Name of the source DataFrame. Yes NA outputMode Output mode of the stream. Yes NA options Options for the Kafka Stream. Yes NA"},{"location":"implementations/spark/operators/spark/sinks/stream/SocketStreamWriter/","title":"Socket Stream Writer","text":""},{"location":"implementations/spark/operators/spark/sources/BatchSink/","title":"Batch Sink (Generic Operator)","text":"<p>Writes a dataframe to a location in a specific format.</p>"},{"location":"implementations/spark/operators/spark/sources/BatchSink/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name BatchSink com.intuit.data.simplan.spark.core.operators.sinks.batch.BatchSink"},{"location":"implementations/spark/operators/spark/sources/BatchSink/#configuration","title":"Configuration","text":"<pre><code>WriteData {\naction {\noperator = BatchSink\nconfig = {\nsource = sourceDataFrame\nformat = &lt;PARQUET | AVRO | ORC | JSON | CSV | TEXT | JDBC&gt;\nlocation = /path/to/file/or/directory\nrepartition = {\nenabled = true\nnumberOfPartitions = 10\ncolumns = [\"col1\", \"col2\"]\nhint = \"tableName or /path/to/file/or/directory\"\noptimalFileSize = 128\ndefaultNumOfPartitions = 10\n}\npartitionBy = [\"col1\", \"col2\"]\noptions = {\n\"option1\" = \"value1\"\n}\n}\n}\n}\n</code></pre>"},{"location":"implementations/spark/operators/spark/sources/BatchSink/#parameters","title":"Parameters","text":"Parameter Description Required Default source Dataframe to write to the location. Yes NA format Format of the file. No parquet location Path to the file or directory to write to. Yes NA repartition Logically repartition the data before writing. No NA partitionBy Physically Partition the data before writing. No NA options Options to be passed to the writer. No NA"},{"location":"implementations/spark/operators/spark/sources/BatchSink/#repartition-section","title":"Repartition Section","text":"Parameter Description Required Default enabled Enable repartitioning. No false numberOfPartitions Number of partitions to repartition to. No NA columns Columns to repartition on. No NA hint Hint to repartition.  Either a Catelog registered table name or a path to and S3 file can be provided. No NA optimalFileSize Optimal file size to repartition to in MB No 128 defaultNumOfPartitions Default number of partitions to repartition to. No {spark-default}"},{"location":"implementations/spark/operators/spark/sources/BatchSink/#sub-operators","title":"Sub Operators","text":"<p>The following are the sub operators of BatchSource operator. These operators are used to read specific file formats from a location. Instead of specifying the format in the configuration, the sub operator can be used to read the file. All the parameters of the sub operator are same as the parameters of the BatchSource operator.</p> Short Name Fully Qualified Name AvroBatchSink com.intuit.data.simplan.spark.core.operators.sinks.batch.AvroBatchSink ParquetBatchSink com.intuit.data.simplan.spark.core.operators.sinks.batch.ParquetBatchSink JSONBatchSink com.intuit.data.simplan.spark.core.operators.sinks.batch.JSONBatchSink CSVBatchSink com.intuit.data.simplan.spark.core.operators.sinks.batch.CSVBatchSink JDBCBatchSink com.intuit.data.simplan.spark.core.operators.sinks.batch.JDBCBatchSink KafkaBatchSink com.intuit.data.simplan.spark.core.operators.sinks.batch.KafkaBatchSink"},{"location":"implementations/spark/operators/spark/sources/BatchSource/","title":"Batch Source (Generic Operator)","text":"<p>Reads a </p>"},{"location":"implementations/spark/operators/spark/sources/BatchSource/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name BatchSource com.intuit.data.simplan.spark.core.operators.sources.batch.BatchSource"},{"location":"implementations/spark/operators/spark/sources/BatchSource/#configuration","title":"Configuration","text":"<pre><code>ReadDataToADataframe {\naction {\noperator = BatchSource\nconfig = {\nlocation = /path/to/file\nformat = &lt;PARQUET | AVRO | ORC | JSON | CSV | TEXT | JDBC&gt;\nschema = schemaJson(\"/path/to/schema.json\")\ntableType = TEMP | MANAGED | NONE\ntable = \"table_name\"\nprojection = [\"col1\", \"col2\"]\nfilter = \"col1 &gt; 10\"\noptions = {\n\"option1\" = \"value1\"\n}\ndirectorySortPattern = \"[\\\\d]+~[\\\\d]+\"\n}\n}\n}\n</code></pre>"},{"location":"implementations/spark/operators/spark/sources/BatchSource/#parameters","title":"Parameters","text":"Parameter Description Required Default location Path to the file or directory to read from. Yes NA format Format of the file. No parquet schema Schema of the records in the file. No NA tableType Type of table that gets created.  NONE - Maintained in memory as dataframe(Not usable in SQL) MANAGED - As managed table in Hive  TEMP - Temperory table in Memory(Usable in SQL) No NONE table Name of the table to be created, if MANAGED is selected as tableType No {task-name} projection List of columns to be projected. No NA filter Filter condition to be applied. No NA options Options to be passed to the reader. No NA directorySortPattern Regex pattern to sort the files in the directory. No NA"},{"location":"implementations/spark/operators/spark/sources/BatchSource/#sub-operators","title":"Sub Operators","text":"<p>The following are the sub operators of BatchSource operator. These operators are used to read specific file formats from a location. Instead of specifying the format in the configuration, the sub operator can be used to read the file. All the parameters of the sub operator are same as the parameters of the BatchSource operator.</p> Short Name Fully Qualified Name AvroBatchSource com.intuit.data.simplan.spark.core.operators.sources.batch.AvroBatchSource ParquetBatchSource com.intuit.data.simplan.spark.core.operators.sources.batch.ParquetBatchSource ORCBatchSource com.intuit.data.simplan.spark.core.operators.sources.batch.ORCBatchSource JSONBatchSource com.intuit.data.simplan.spark.core.operators.sources.batch.JSONBatchSource CSVBatchSource com.intuit.data.simplan.spark.core.operators.sources.batch.CSVBatchSource TextBatchSource com.intuit.data.simplan.spark.core.operators.sources.batch.TextBatchSource JDBCBatchSource com.intuit.data.simplan.spark.core.operators.sources.batch.JDBCBatchSource"},{"location":"implementations/spark/operators/spark/sources/DeltaTableSource/","title":"Avro Batch Source","text":"<p>Reads Parquet records from a location set and converts each record into a Structured Record. The schema of the Structured Record is derived from the Parquet schema. If the Parquet schema is not available, it can be provided using the schema property. Schema is provided as Schema Qualifed Param.</p>"},{"location":"implementations/spark/operators/spark/sources/DeltaTableSource/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name DeltaTableSource com.intuit.data.simplan.spark.core.operators.sources.DeltaTableSource"},{"location":"implementations/spark/operators/spark/sources/DeltaTableSource/#configuration","title":"Configuration","text":"<pre><code>ReadData {\naction {\noperator = DeltaTableSource\nconfig = {\npath = /path/to/file\nschema = schemaJson(\"/path/to/schema.json\")\nautoCreate = true\n}\n}\n}\n</code></pre>"},{"location":"implementations/spark/operators/spark/sources/DeltaTableSource/#parameters","title":"Parameters","text":"Parameter Description Required Default path Path to the file or directory to read from. Yes NA schema Uses this schema to create table if table does not exist No NA autoCreate If true, creates the table if it does not exist. No true"},{"location":"implementations/spark/operators/spark/sources/stream/FileStreamingSource/","title":"File Streaming Source","text":"<p>Reads records from a File as a Stream and converts each record into a Structured Record. The schema of the Structured Record can be provided using the schema property. Schema is provided as Schema Qualifed Param.</p>"},{"location":"implementations/spark/operators/spark/sources/stream/FileStreamingSource/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name FileStreamingSource com.intuit.data.simplan.spark.core.operators.sources.stream.FileStreamingSource"},{"location":"implementations/spark/operators/spark/sources/stream/FileStreamingSource/#configuration","title":"Configuration","text":"<pre><code>    ReadStream {\naction {\noperator = FileStreamingSource\nconfig = {\nformat = &lt;JSON, AVRO, PARQUET, ORC, CSV, TEXT&gt;\nschema = schemaJson(\"/path/to/schema.json\")\ntableType = &lt; NONE | TEMP&gt;\nwatermark = {\neventTime = fieldName\ndelayThreshold = \"1 minute\"\n}\noptions = {\n&lt;options1&gt; = &lt;value1&gt;\n}\n}\n}\n}\n</code></pre>"},{"location":"implementations/spark/operators/spark/sources/stream/FileStreamingSource/#parameters","title":"Parameters","text":"Parameter Description Required Default format Format of the file. Yes NA schema Schema of the records in the file. Yes NA tableType Type of table that gets created.  NONE - Maintained in memory as dataframe(Not usable in SQL) TEMP - Temperory table in Memory(Usable in SQL) No NONE watermark Watermark configuration. No NA options Options to be passed to the file reader.  Any valid spark property can be passed depending on the file format you wish to read No NA"},{"location":"implementations/spark/operators/spark/sources/stream/KafkaStreamingSource/","title":"Kafka Stream Reader","text":"<p>Reads records from a Kafka Stream and converts each record into a Structured Record. The schema of the Structured Record can be provided using the schema property. Schema is provided as Schema Qualifed Param. </p>"},{"location":"implementations/spark/operators/spark/sources/stream/KafkaStreamingSource/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name KafkaStreamingSource com.intuit.data.simplan.spark.core.operators.sources.stream.KafkaStreamingSource"},{"location":"implementations/spark/operators/spark/sources/stream/KafkaStreamingSource/#configuration","title":"Configuration","text":"<pre><code>    ReadVendorStream {\naction {\noperator = KafkaStreamingSource\nconfig = {\nformat = JSON\npayloadSchema = schemaJson(\"path/to/payloadSchema.schema.json\")\nheaderSchema = schemaJson(\"path/to/headerSchema.schema.json\")\nheaderField = \"key\"\ntableType = &lt;TEMP | NONE&gt;\nwatermark ={\neventTime = fieldName\ndelayThreshold = \"1 minute\"\n}\nparseMode = &lt;ALL_PARSED | PAYLOAD_ONLY | HEADER_ONLY | ALL&gt;\noptions = {\n\"kafka.bootstrap.servers\" = \"server1:port, server2:port\"\nincludeHeaders = true\nsubscribe = topic-name\n...\n}\n}\n}\n}\n</code></pre>"},{"location":"implementations/spark/operators/spark/sources/stream/KafkaStreamingSource/#parameters","title":"Parameters","text":"Parameter Description Required Default format Format of the data in the stream. Yes NA payloadSchema Schema of the payload in the stream. Yes NA headerSchema Schema of the header in the stream. No NA headerField Name of the field in the header schema that contains the header. No key tableType Type of table that gets created.  NONE - Maintained in memory as dataframe(Not usable in SQL) TEMP - Temperory table in Memory(Usable in SQL) No NONE watermark Watermark configuration for the stream. No NA parseMode Mode of parsing the stream.  ALL_PARSED - Both header and payload are parsed and available in the dataframe.  PAYLOAD_ONLY - Only payload is parsed and available in the dataframe.  HEADER_ONLY - Only header is parsed and available in the dataframe.  ALL - Both header and payload are parsed and available in the dataframe. No PAYLOAD_ONLY options Options to be passed to the Kafka reader.  Any Kafka property can be passed prefixed with kafka.* Yes NA"},{"location":"implementations/spark/operators/spark/sources/stream/KafkaStreamingSource/#sample-configuration","title":"Sample Configuration","text":"<pre><code>    bands {\n  action {\n    operator = KafkaStreamReader\n    config = {\n      format = JSON\n      payloadSchema = schemaJson(\"schema.json\")\n      headerSchema = schemaJson(\"header.schema.json\")\n      parseMode = ALL_PARSED\n      options = {\n        \"kafka.bootstrap.servers\" = \"b-3.eventbusmsk-sbseg-e2e.hz7ee1.c3.kafka.us-west-2.amazonaws.com:9094, b-4.eventbusmsk-sbseg-e2e.hz7ee1.c3.kafka.us-west-2.amazonaws.com:9094, b-2.eventbusmsk-sbseg-e2e.hz7ee1.c3.kafka.us-west-2.amazonaws.com:9094\"\n        \"kafka.cluster.env\" = \"e2e\"\n        \"kafka.cluster.name\" = \"ebus_sbseg_sl\"\n        \"kafka.cluster.region\" = \"uw2\"\n        \"kafka.group.id\" = \"test111\"\n        \"kafka.auto.commit.interval.ms\" = \"1000\"\n        \"kafka.ssl.keystore.location\" = \"/Users/tabraham1/Intuit/Development/certifates/local-certificate.jks\"\n        \"kafka.ssl.keystore.password\" = \"&lt;password&gt;\"\n        \"kafka.security.protocol\" = \"ssl\"\n        \"includeHeaders\" = \"true\"\n        subscribe = \"e2e.qbo.commerce.vendormanagement.vendor.v1\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"implementations/spark/operators/spark/sources/stream/SocketStreamingSource/","title":"Socket Stream Reader","text":"<p>Reads records from a Socket Stream and converts each record into a Structured Record. The schema of the Structured Record can be provided using the schema property. Schema is provided as Schema Qualifed Param.</p> <pre><code>    ReadStream {\n      action {\n        operator = com.intuit.data.simplan.spark.core.operators.sources.stream.SocketStreamingSource\n        config = {\n          format = &lt;Json, Avro, Protobuf &gt;\n          payloadSchema = /path/to/payloadschemafile\n          headerSchema = /path/to/headerschemafile\n          parseMode = ALL_PARSED\n          options = {\n            &lt;options1&gt; = &lt;value1&gt;\n          }\n        }\n      }\n    }\n</code></pre> <p><code>Format</code> = to define serialization format. It can be either json, avro or protobuf <code>payloadSchema</code> = path to your schema file <code>headerSchema</code> = schema for your header file <code>parseMode</code> =  different kind of parsing options</p> <p>Example for reading from socket stream</p> <pre><code>    ReadStream {\n      action {\n        operator = com.intuit.data.simplan.spark.core.operators.sources.stream.SocketStreamingSource\n        config = {\n          format = JSON\n          //payloadSchema = schemaDDL(\"someNumber LONG, anotherNumber LONG\")\n          parseMode = COMPLETE\n          options = {\n            host = localhost\n            port = 12345\n          }\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/spark/operators/spark/transformations/DeltaTableMerge/","title":"Delta Table Merge Transformation","text":"<p>The Delta Table Merge transformation allows you to merge data from a Delta Lake table into another Delta Lake table. The transformation is based on the Delta Table Merge operation.</p>"},{"location":"implementations/spark/operators/spark/transformations/DeltaTableMerge/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name DeltaStreamingSink com.intuit.data.simplan.spark.core.operators.sinks.stream.DeltaStreamingSink"},{"location":"implementations/spark/operators/spark/transformations/DeltaTableMerge/#configuration","title":"Configuration","text":"<pre><code>    deltaWrite {\n      action {\n        operator = DeltaStreamingSink\n        config = {\n          outputMode = append\n          source = formatEvents\n          forEachBatch {\n            handler = com.intuit.data.simplan.spark.core.handlers.foreachbatch.DeltaMergeForEachBatchHandler\n            config = {\n              deltaTableSource = live_table\n              deltaEventsSource = formatEvents\n              deltaTableSourceAlias = \"live_table\"\n              deltaEventsSourceAlias = \"events\"\n              mergeCondition = \"live_table.id = events.id\"\n              matchCondition = [\n                {\n                  \"expression\" = \"update condition\"\n                  \"action\" = \"UPDATE\"\n                },\n                {\n                  \"expression\" = \"delete condition\"\n                  \"action\" = \"DELETE\"\n                }\n              ],\n              notMatchCondition = [\n                {\n                  \"expression\" = \"insert condition\"\n                  \"action\" = \"INSERT\"\n                }\n              ]\n            }\n          }\n          options {\n            path = /Users/tabraham1/Intuit/Development/WorkSpace/Simplan/Workspace/deltaTable\n            checkpointLocation = /Users/tabraham1/Intuit/Development/WorkSpace/Simplan/Workspace/deltaTable/checkpoint\n          }\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/spark/operators/spark/transformations/DropDuplicates/","title":"Drop duplicates Transformation","text":"<p>Drop Duplicates is a transformation that allows you to remove duplicate rows from a a set of rows.</p>"},{"location":"implementations/spark/operators/spark/transformations/DropDuplicates/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name DropDuplicatesOperator com.intuit.data.simplan.spark.core.operators.transformations.DropDuplicatesOperator"},{"location":"implementations/spark/operators/spark/transformations/DropDuplicates/#compatability","title":"Compatability","text":"Batch Streaming Yes Yes"},{"location":"implementations/spark/operators/spark/transformations/DropDuplicates/#configuration","title":"Configuration","text":"<pre><code>    DropDuplicateVendorEvents {\n      action {\n        operator = DropDuplicatesOperator\n        config = {\n          source = ProjectRequiredFields\n          primaryKeyColumns = [fieldNamew]\n          dropWindowDuration = \"10 minutes\"\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/spark/operators/spark/transformations/DropDuplicates/#parameters","title":"Parameters","text":"Parameter Description Required Default source Name of the source DataFrame. Yes NA projections List of columns to be selected. Yes NA"},{"location":"implementations/spark/operators/spark/transformations/Filtering/","title":"Filtering Transformation","text":"<p>Filtering is a transformation that allows you to filter out rows from a DataFrame based on a condition. The condition can be a SQL expression which leads to a boolean outcome.</p>"},{"location":"implementations/spark/operators/spark/transformations/Filtering/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name FilteringOperator com.intuit.data.simplan.spark.core.operators.transformations.FilteringOperator"},{"location":"implementations/spark/operators/spark/transformations/Filtering/#compatability","title":"Compatability","text":"Batch Streaming Yes Yes"},{"location":"implementations/spark/operators/spark/transformations/Filtering/#configuration","title":"Configuration","text":"<pre><code> Filtering {\n      action {\n        operator = FilteringOperator\n        config = {\n          source = SourceDataFrame\n          condition = \"event_type = 'EventToBeFiltered'\"\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/spark/operators/spark/transformations/Filtering/#parameters","title":"Parameters","text":"Parameter Description Required Default source Name of the source DataFrame. Yes NA condition SQL expression that leads to a boolean outcome. Yes NA"},{"location":"implementations/spark/operators/spark/transformations/GroupedAggregationOperator/","title":"Grouped Aggregation Transformation","text":"<p>Grouped aggregation is a transformation that allows you to aggregate rows from a DataFrame based on a group by condition. Expressions can be used to aggregate the rows. Any expressions supported by Spark SQL can be used.</p>"},{"location":"implementations/spark/operators/spark/transformations/GroupedAggregationOperator/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name GroupedAggregationOperator com.intuit.data.simplan.spark.core.operators.transformations.GroupedAggregationOperator"},{"location":"implementations/spark/operators/spark/transformations/GroupedAggregationOperator/#compatability","title":"Compatability","text":"Batch Streaming Yes Yes"},{"location":"implementations/spark/operators/spark/transformations/GroupedAggregationOperator/#configuration","title":"Configuration","text":"<pre><code>    GroupedAggregation {\n      action {\n        operator = GroupedAggregationOperator\n        config = {\n          source = sourceDataFrame\n          grouping = [groupField]\n          aggs {\n            aggField1 = aggregationExpression1\n            aggField2 = aggregationExpression2\n          }\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/spark/operators/spark/transformations/GroupedAggregationOperator/#parameters","title":"Parameters","text":"Parameter Description Required Default source Name of the source DataFrame. Yes NA grouping List of fields to group by. Yes NA aggs Map of fields to aggregate and the aggregation expression. Yes NA"},{"location":"implementations/spark/operators/spark/transformations/GroupedAggregationOperator/#example","title":"Example","text":"<p>Calculating 2 aggregations to find <code>numberjobs</code> and <code>totalNumberOfStatements</code> grouped by a field called <code>asset</code>.</p> <pre><code>    NumberOfVendorsLifetime {\n      action {\n        operator = GroupedAggregationOperator\n        config = {\n          source = ProjectRequiredFields\n          grouping = [asset]\n          aggs {\n            numberOfJobs = count(appName)\n            totalNumberOfStatemts = sum(numOfStatements)\n          }\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/spark/operators/spark/transformations/Projections/","title":"Projections Transformation","text":"<p>Projections is a transformation that allows you to select a subset of columns from a DataFrame. The columns can be selected by name or by using a SQL expression.</p>"},{"location":"implementations/spark/operators/spark/transformations/Projections/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name ProjectionOperator com.intuit.data.simplan.spark.core.operators.transformations.ProjectionOperator"},{"location":"implementations/spark/operators/spark/transformations/Projections/#compatability","title":"Compatability","text":"Batch Streaming Yes Yes"},{"location":"implementations/spark/operators/spark/transformations/Projections/#configuration","title":"Configuration","text":"<pre><code>    ProjectRequiredFields {\n      action {\n        operator = ProjectionOperator\n        config = {\n          source = SourceDataFrame\n          projections = [\n            \"value.id.accountId as accountId\",\n            \"value.id.entityId as entityId\",\n            \"value.active as active\",\n            \"headers.entityChangeAction as entityChangeAction\",\n            \"headers.idempotenceKey as idempotenceKey\",\n            \"current_timestamp() as timestamp\"\n          ]\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/spark/operators/spark/transformations/Projections/#parameters","title":"Parameters","text":"Parameter Description Required Default source Name of the source DataFrame. Yes NA projections List of columns to be selected. Yes NA"},{"location":"implementations/spark/operators/spark/transformations/SparkSql/","title":"SQL Transformations","text":"<p>SqlStatementExecutor is a Spark SQL transformation that executes a Spark SQL statement on a DataFrame. It is a wrapper around the Spark SQL API. Any Spark SQL statement can be executed using this transformation. Any existing table in Hive metastore can be queried using this transformation. The transformation can also be used to create a new table in Hive metastore.</p>"},{"location":"implementations/spark/operators/spark/transformations/SparkSql/#operator-definition","title":"Operator Definition","text":"Short Name Fully Qualified Name SqlStatementExecutor com.intuit.data.simplan.spark.core.operators.transformations.SqlStatementOperator"},{"location":"implementations/spark/operators/spark/transformations/SparkSql/#compatability","title":"Compatability","text":"Batch Streaming Yes Yes"},{"location":"implementations/spark/operators/spark/transformations/SparkSql/#configuration","title":"Configuration","text":"<pre><code>    TaskName {\n      action {\n        operator = SqlStatementExecutor\n        config = {\n            table = \"table_name\"\n            tableType = &lt;TEMP | MANAGED | NONE&gt;\n            sql = \"SELECT field FROM schema.table_name\"\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/spark/operators/spark/transformations/SparkSql/#parameters","title":"Parameters","text":"Parameter Description Required Default table Name of the table to be created. No NA tableType Type of table that gets created.  NONE - Maintained in memory as dataframe(Not usable in SQL) MANAGED - As managed table in Hive  TEMP - Temperory table in Memory(Usable in SQL) No NONE sql Spark SQL statement to be executed. Yes NA"},{"location":"implementations/spark/operators/spark/writers/batch/DeltaBatchSink/","title":"Delta Batch Sink","text":"<p>For Writing to Batch Sink</p> <pre><code>ReadData {\n  action {\n    operator = com.intuit.data.simplan.spark.core.operators.sources.batch.DeltaBatchSink\n    config = {\n      source = &lt;file path or previoue operator&gt;\n      location = /output/path\n      options ={\n        &lt;option1&gt; = &lt;value1&gt;\n      }\n    }\n  }\n}\n</code></pre> <p>All options supported by spark can be used</p> <p>Example for writing to Delta Batch Sink</p> <pre><code>    finalOutput {\n      action {\n        operator = DeltaBatchSink\n        config = {\n          source = SqlOperation\n          location = ${simplan.variables.configBasePath}/output/consolidated/\n        }\n      }\n    }\n</code></pre>"},{"location":"implementations/spark/qualified_param/schema_qualified_param/","title":"Schema Qualified Param","text":"<p>Coming soon...</p>"},{"location":"implementations/spark/system_configurations/spark_properties/","title":"Spark Properties","text":"<p>Simplan allows you to configure Spark properties for your Spark application. You can configure Spark properties in the following way.</p> <p>Limitations</p> <p>Only spark properties that can be modified after SparkContext is created can be configured using Simplan.</p> <p>For example, you can configure <code>spark.serializer</code> but not <code>spark.master</code> or <code>spark.driver.memory</code> using this method.</p> <pre><code>simplan {\nsystem {\nconfig {\nspark {\nproperties {\n\"spark.serializer\" = org.apache.spark.serializer.KryoSerializer\nkey1 = value1\n...\n...\nkeyN = valueN\n}\n}\n}\n}\n}\n</code></pre>"},{"location":"overview/GettingStarted/","title":"Getting Started","text":"<p>Simplan is a framework for defining an execution plan for operators that can be executed on different execution engines like Spark, Flink, Storm, Beam or even run as a simple console Application. It provides a common abstraction to define pluggable operators for processing both batch and streaming data and run on different execution engines with ability to integrate with various source/sinks. There are different execution engine implementations of Simplan available. Please select the following link to get started with the execution engine of your choice.</p>"},{"location":"overview/GettingStarted/#simplan-implementations","title":"Simplan Implementations","text":"<ul> <li>Simplan Spark</li> <li>Simplan Flink</li> <li>Simplan Presto</li> </ul>"},{"location":"overview/implementations/console/","title":"Console","text":""},{"location":"overview/implementations/console/#console-application-on-simplan","title":"Console Application on Simplan","text":"<p>Simplan Console is a simple console application that can be used to run operators as a console application. It can be used to run any non-data operators which doesn't need an execution engine as a simple java application. If you can express your operator as a java class, you can run it as a console application using Simplan Console.</p> <p>Example Superglue uses Simplan to schedule all its pipelines with databricks.</p> <p>Repo: https://github.intuit.com/Superglue/sg-client</p>"},{"location":"overview/implementations/dataprocessing/data-processing/","title":"Simplan for Data Process Authoring","text":"<p>For generating customer value from data, Data workers need to process large volumes of batch and streaming data. Separate codebase are maintained for Batch and Streaming modes which leads to siloed implementations for common data processing patterns. This leads to duplicate efforts from implementation to maintenance, hampering productivity. </p> <p>Users will be able to provide business logic as operators in a config file and the framework will take care of the rest. The framework will take care of the execution of these operators and provide the results to the user. The framework will also provide the lineage of the data and the metrics of the execution. </p>"},{"location":"overview/implementations/dataprocessing/data-processing/#simplam-tech-stack","title":"Simplam Tech Stack","text":"<p>Simplan framework offers a bunch of built-in operators for common processing tasks. These operators can be used as is or can be extended to add custom logic. The framework also provides a way to write custom operators.</p>"},{"location":"overview/implementations/dataprocessing/data-processing/#featuresbenefits","title":"Features/Benefits","text":"<ul> <li>Config Driven (Low/No code)</li> <li>Pluggable/Reusable operators for common processing tasks</li> <li>Multiple Execution Runtimes, Spark, Flink, Presto</li> <li>Abstraction over Execution Runtimes like Spark, Flink etc</li> <li>Batch and Streaming workloads</li> <li>External Integrations : Redshift, Athena, Kafka etc</li> <li>Built-In Quality control with circuit breakers.</li> <li>Lineage, Observability, and Metrics tracking.</li> <li>Integration for Intuit services like IDPS, Config Services, etc</li> <li>Improves developer productivity by 10-100 times</li> <li>Improves code quality, maintainability and reduces duplication</li> </ul>"},{"location":"overview/implementations/dataprocessing/data-processing/#data-processing-implementations","title":"Data Processing Implementations","text":""},{"location":"overview/implementations/dataprocessing/data-processing/#simplan-spark-successor-to-quicketl","title":"Simplan Spark (Successor to QuickETL)","text":"<p>Simplan Spark is an implementation of Simplan framework for Apache Spark execution engine. It provides a way to author batch and streaming operations in a simple way.</p> <p>Learn more : Simplan Spark</p>"},{"location":"overview/implementations/dataprocessing/data-processing/#simplan-flink","title":"Simplan Flink","text":"<p>Learn More : Simplan Flink</p>"},{"location":"overview/implementations/dataprocessing/data-processing/#simplan-presto","title":"Simplan Presto","text":"<p>Learn More : Simplan Presto</p>"},{"location":"overview/implementations/dataprocessing/flink/","title":"Flink","text":""},{"location":"overview/implementations/dataprocessing/flink/#simplan-for-apache-flink","title":"Simplan for Apache Flink","text":"<p>Simplan for Apache Flink is a framework for defining an execution plan for operators that can be executed on Apache Flink. It provides a common abstraction to define pluggable operators for processing both batch and streaming data and run on Apache Flink with ability to integrate with various source/sinks.</p> <p>For flink documentation, please visit Simplan Flink</p>"},{"location":"overview/implementations/dataprocessing/presto/","title":"Presto","text":""},{"location":"overview/implementations/dataprocessing/presto/#simplan-for-presto","title":"Simplan for Presto","text":"<p>Simplan for Presto is a framework for defining an execution plan for operators that can be executed on Presto. It provides a common abstraction to define pluggable operators for processing both batch and streaming data and run on Presto with ability to integrate with various source/sinks.</p> <p>For Presto documentation, please visit Simplan Presto</p>"},{"location":"overview/implementations/dataprocessing/spark/","title":"Spark","text":""},{"location":"overview/implementations/dataprocessing/spark/#simplan-for-apache-spark","title":"Simplan for Apache Spark","text":"<p>Simplan for Apache Spark is a framework for defining an execution plan for operators that can be executed on Apache Spark. It provides a common abstraction to define pluggable operators for processing both batch and streaming data and run on Apache Spark with ability to integrate with various source/sinks.</p> <p>For Spark documentation, please visit Simplan Spark</p>"},{"location":"overview/implementations/logging/Simplan%20Fields/","title":"Simplan Fields","text":""},{"location":"overview/implementations/logging/Simplan%20Fields/#simplan-logging","title":"Simplan Logging","text":"<p>Simplan Logging Fields</p> Field Discription"},{"location":"overview/implementations/logging/Simplan%20Logging/","title":"Simplan Logging","text":""},{"location":"overview/implementations/logging/Simplan%20Logging/#simplan-logging","title":"Simplan Logging","text":"<p>Simplan flow and logging events</p> <p></p> <p>Simplan flow and logging events</p> <p></p>"},{"location":"overview/implementations/orchestration/orchestration/","title":"Simplan for Orchestrating Data Processing Jobs","text":"<p>Simplan framework is used to orchestrate all the jobs in Superglue. SG-Client is a simplan based appliation which is used to orchestrate all the jobs in Superglue.</p>"}]}